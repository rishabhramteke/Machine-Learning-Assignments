\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{gensymb}
\usepackage{amsmath}

\title{GNR652 Assisgnment 2 \\}
\author{Rishabh Ramteke - 170070046 }


\begin{document}

% make a title page.
\maketitle
\section{Introduction}
A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.

\section{Preliminary}
A classification problem has two types of variables, $x_i$ $\epsilon$ $R^d$, the vector of observations(features) in the
world, and $y_i$ $\epsilon$ $R^d$
state(class) vector of the world. The set of all samples in this world (or dataset) constructs the observation set X $\epsilon$ $R^{nxd}$ and state set Y $\epsilon$ $R^{nxd}$. The task of our machine learning algorithm for classification is to find a mapping x $\rightarrow$ y that achieves smallest number of misclassification. To
quantify the misclassification error, we define a loss L[y, f(x, $\alpha$)] where f(x, $\alpha$) is the function of mapping
parametrized by $\alpha$.

\section{Theory}

\subsection{Linear SVMs}
  \begin{itemize}
  \item Training data $\{\mathbf x_i, y_i\}\ i = 1,\ldots,l$, 
    $\mathbf    x_i \in \mathbb{R}^n$, and $y_i \in \{ -1, 1\}$
  \item On a separating hyperplane: $\mathbf w^T \mathbf x + b = 0$, where
    \begin{itemize}
    \item $w$ normal to the hyperplane
    \item $\displaystyle\frac{|b|}{\|\mathbf w\|}$ is the distance to origin
    \item $\|\mathbf w\|$ Euclidean norm of $\mathbf w$
    \end{itemize}
  \end{itemize}
  \begin{itemize}
      \item $d_+$, $d_-$ shortest distances from labeled points to hyperplane
  \item Define margin $m = d_+ + d_-$
  \item Task: find the separating hyperplane that maximizes $m$
  \item Key point: Maximizing the margin minimizes the VC dimension
  
  \end{itemize}
  \subsection{Computation}
  \begin{itemize}
  \item For the separating plane:
    \begin{eqnarray}
      \mathbf w^T \mathbf x_i + b \geq +1,& y_i = +1\\
      \mathbf w^T \mathbf x_i + b \leq -1,& y_i = -1\\
      \equiv\\
      y_i(\mathbf w^T \mathbf x_i + b) - 1 \geq 0,& \forall i
    \end{eqnarray}
  \item For the closest points the equalities are satisfied, so:
    \begin{equation}
      d_+ + d_- = \frac{|1-b|}{\|w\|} + \frac{|-1-b|}{\|w\|} = \frac{2}{\|w\|}
    \end{equation}
 
  \end{itemize}


\subsection{Switching to Lagrangian}
  \begin{itemize}
  \item One coefficient per train sample
  \item The constraints easier to handle
  \item Training data appears only in dot products
  \item Great for applying the kernel trick later on
  \end{itemize}


\subsection{Lagrangian Form}
  \begin{itemize}
  \item Minimize
    \begin{equation}
      L_P = \frac{\|w\|^2}{2} - \displaystyle\sum_{i=1}^l \alpha_i y_i (\mathbf x_i \mathbf x_i + b) + \displaystyle\sum^l_{i=1} \alpha_i
    \end{equation}
  \item Convex quadratic programming problem with the dual: maximize
    \begin{equation}
      L_D = \displaystyle\sum^l_{i=1} \alpha_i - \frac{1}{2}\displaystyle\sum_{i,j=1}^l \alpha_i \alpha_j y_i y_j (\mathbf w^T \mathbf x_j)
    \end{equation}
  \end{itemize}


\subsection{The Support Vectors}
  \begin{itemize}
  \item The points with $\alpha_i > 0$ are the support vectors
  \item Solution only depends on them
  \item All  others have $\alpha_i =  0$ and can  be moved arbitrarily
    far from the decision hyperplane, or removed
  \end{itemize}


\subsection{Testing}
  \begin{itemize}
  \item Once the hyperplane is found:
    \begin{equation}
      \hat y = \sgn(\mathbf w^T \mathbf x + b)
    \end{equation}
  \end{itemize}

\section{Implementing the SVM algorithm (Hard margin)}
\subsection{Aim and Methods}
\begin{itemize}
    \item The dataset is for credit card fraud. It is a binary classification task (labels 0 and 1).
    \item Since the labels are 0 and 1 , we need to change them to -1 and 1 respectively as these are preferred in SVM.
    \item Now, since the data is highly biased (500 '1' labels among 2 lakh '-1' labels), thus I will consider 100 '+1' and 100 '-1'.
    \item In the dual formulation based SVM, we convert the primal Lagrangian to the dual formulation which is a convex quadratic optimization problem with linear constraints. We apply a convex optimization function \boldsymbol{"cvxopt.solvers.qp"} (available in python) to solve the $\alpha$ values (the lagrangian multipliers).
    \item Note that if there are N data points in the training set, $\alpha$ will be a vector of 1 x N (corresponding to the points). Many of these N $\alpha$ values will be 0 and the ones corresponding to support vector will be non-zero.
    \item We need to convert the dual formula in the matrix notation.
    \item "cvxopt.solvers.qp" requires a particular form for the optimisation problem. So need to convert the matrix based dual formula into the one "cvxopt" accepts. It will give output $\alpha$from which we can calculate w and b parameter.
    \item  Check the testing accuracy based on the \% of correctly classified samples in the test set. 
    
\end{itemize}
\subsection{Linearly separable, binary classification}
General steps to solve the SVM problem are the following:

\begin{itemize}
\item Create P where $H_{i,j} = y^{(i)}y^{(j)} <x^{(i)} x^{(j)}>$
\item Calculate $\mathbf{w} = \sum_i^m y^{(i)} \alpha_i x^{(i)}$
\item Determine the set of support vectors S by finding the indices such that $\alpha_i > 0$
\item Calculate the intercept term using b = $y^{(s)} - \sum_{m \in S} \alpha_m  y^{(m)}  <x^{(m)}  x^{(s)} >$
\item For each new point x′ classify according to y' = $ sign(w^T x' + b)$
\end{itemize}

%\displaystyle f(x) = \begin{cases}
% 1 & \text{if \quad $w^T x + b > 0$} \\ 
% -1 & \text{if \quad $w^T x + b < 0$} 
% \end{cases}




\subsection{Re-writing the problem in an appropriate format}
Since we will solve this optimization problem using the CVXOPT library in python we will need to match the solver's API which, according to the documentation is of the form:


\begin{itemize}

\item $\begin{aligned}
    & \min \frac{1}{2} x^TPx + q^Tx
    \\
     s.t. \ & \ Gx \leq h 
    \\
    & \ Ax = b
\end{aligned}$
\item With API \\
cvxopt.solvers.qp(P, q[, G, h[, A, b[, solver[, initvals]]]])



\item the dual problem is expressed as:
$$\max_{\alpha} \sum_i^m \alpha_i - \frac{1}{2} \sum_{i,j}^m y^{(i)}y^{(j)} \alpha_i \alpha_j <x^{(i)} x^{(j)}>$$

\item Let H be a matrix such that $ H_{i,j} = y^{(i)}y^{(j)} <x^{(i)} x^{(j)}>$  then the optimization becomes:
$\begin{aligned}
    & \max_{\alpha} \sum_i^m \alpha_i  - \frac{1}{2}  \alpha^T \mathbf{H}  \alpha
    \\
     s.t. & \ \alpha_i \geq 0 
    \\
    &  \ \sum_i^m \alpha_i y^{(i)} = 0  
\end{aligned}$

\item We convert the sums into vector form and multiply both the objective and the constraint by −1 which turns this into a minimization problem and reverses the inequality

$\begin{aligned}
    & \min_{\alpha}  \frac{1}{2}  \alpha^T \mathbf{H}  \alpha - 1^T \alpha
    \\
    & s.t. \ - \alpha_i \leq 0 
    \\
    & s.t. \ y^T \alpha = 0 
\end{aligned}$

\item We are now ready to convert our numpy arrays into the cvxopt format, using the same notation as in the documentation this gives

\item P:=H a matrix of size m×m
q:=−1⃗  a vector of size m×1
G:=−diag[1] a diagonal matrix of -1s of size m×m
h:=0⃗  a vector of zeros of size m×1
A:=y the label vector of size m×1
b:=0 a scalar
\end{itemize}

\subsection{Computing the matrix H in vectorized form}

Consider the simple example with 2 input samples 

$\{x^{(1)}, x^{(2)}\} \in \mathbb{R}^2$
which are two dimensional vectors. i.e.
$ x^{(1)} = (x_1^{(1)} , x_2^{(1)})^T$

$X = \begin{bmatrix} x_1^{(1)} & x_2^{(1)} \\ x_1^{(2)} & x_2^{(2)} \end{bmatrix} \ \ \ y = \begin{bmatrix} y^{(1)}  \\ y^{(2)} \end{bmatrix}$

We now proceed to creating a new matrix X' where each input sample x is multiplied by the corresponding output label y. This can be done easily in Numpy using vectorization and padding.
$X' = \begin{bmatrix} x^{(1)}_1 y^{(1)} & x^{(1)}_2y^{(1)} \\
x^{(2)}_1y^{(2)} & x^{(2)}_2y^{(2)} \end{bmatrix}$

Finally we take the matrix multiplication of X′ and its transpose giving
$H = X'  X'^T$

$H = X'  X'^T = \begin{bmatrix} x^{(1)}_1 y^{(1)} & x^{(1)}_2y^{(1)} \\
x^{(2)}_1y^{(2)} & x^{(2)}_2y^{(2)} \end{bmatrix} \begin{bmatrix} x^{(1)}_1 y^{(1)} & x^{(2)}_1 y^{(2)}  \\
x^{(1)}_2y^{(1)} & x^{(2)}_2y^{(2)} \end{bmatrix}$


\section{Implementation in Python}
\subsection{Importing data and dividing into training and test set}
\begin{verbatim}
#Importing Libraries
import numpy as np
import csv
import matplotlib.pyplot as plt
from numpy.linalg import inv
from cvxopt import matrix as cvxopt_matrix #Importing with custom names to avoid issues with numpy matrix
from cvxopt import solvers as cvxopt_solvers

#Importing data
filepath ="/home/rishabh/Downloads/set1.csv"
File = np.genfromtxt(filepath, delimiter=',') #converting csv to numpy
# X_1 = File[:98, 0:30]
# X_2 = File[98:, 0:30]
np.random.shuffle(File) #shuffle data in File

#defining training and testing set
X_train = File[:180, 0:30]
Y_train = File[:180, [30]]
X_test = File[180:, 0:30]
Y_test = File[180:, [30]]

#Data shapes
print("\n####  Data Shapes  ####\n")
print("MyData_shape =",File.shape)
print("X_train_shape =",X_train.shape,"Y_train_shape =",Y_train.shape)
print("X_test_shape =",X_test.shape,"X_test_shape =",Y_test.shape)

\end{verbatim}
\subsection{CVXOPT solver and resulting $\alpha$}
\begin{verbatim}
# Initializing values and computing H.
m, n = X_train.shape
y = Y_train.reshape(-1, 1) * 1.
X_dash = y * X_train
H = np.dot(X_dash, X_dash.T) * 1.

# Converting into cvxopt format
P = cvxopt_matrix(H)
q = cvxopt_matrix(-np.ones((m, 1)))
G = cvxopt_matrix(-np.eye(m))
h = cvxopt_matrix(np.zeros(m))
A = cvxopt_matrix(y.reshape(1, -1))
b = cvxopt_matrix(np.zeros(1))

# Setting solver parameters 
cvxopt_solvers.options['show_progress'] = False
cvxopt_solvers.options['abstol'] = 1e-10
cvxopt_solvers.options['reltol'] = 1e-10
cvxopt_solvers.options['feastol'] = 1e-10

# Run solver
sol = cvxopt_solvers.qp(P, q, G, h, A, b)
alphas = np.array(sol['x']) #alphas are langrange multipliers
\end{verbatim}
\subsection{Compute w and b parameters}
\begin{verbatim}
#Computing W parameter
w = np.multiply(alphas, Y_train)
w = w.transpose().dot(X_train)

#computing b parameter

#Method 1
# l = w.dot(X_1.transpose())
# z = w.dot(X_2.transpose())
# c1 = l.min(1)
# c2 = z.max(1)
# b = (-0.5) * (c1 + c2)

#Method 2
S = (alphas > 1e-40).flatten()
b = y[S] - np.dot(X_train[S], w.T)

#parameter shapes
print("\n####  Parameter Shapes  ####\n")
print("#### alphas shape =",alphas.shape)
print("#### W shape =",w.shape)
print("#### b shape =",b.shape)

#Display results
print("\n####  Display results  ####\n")
print('Alphas = ',alphas[alphas > 1e-10])
print('w = ', w.flatten())
print('b = ', b[0])
\end{verbatim}
\subsection{Testing and finding accuracy}
\begin{verbatim}
    #Accuracy
for i in range(f):
    if k[0][i] >= 0:
        c[i] = 1
    else:
        c[i] = 0
#print(c)
ans = np.mean(c) * 100
print("\n####  Testing Data  ####\n")
print("#### Accuracy: " + str(ans))
\end{verbatim}
\subsection{Result}
\begin{figure}[H]
   \includegraphics[width=1.5\textwidth]{set1.png}
    \caption{Accuracy for dataset 1}
\end{figure}
\begin{itemize}
    \item Run the code "gnr2.py" on terminal 
    \item I got the following output :
\end{itemize}
\begin{verbatim}
  ####  Data Shapes  ####

('MyData_shape =', (195, 31))
('X_train_shape =', (180, 30), 'Y_train_shape =', (180, 1))
('X_test_shape =', (15, 30), 'X_test_shape =', (15, 1))

####  Parameter Shapes  ####

('#### alphas shape =', (180, 1))
('#### W shape =', (1, 30))
('#### b shape =', (180, 1))

####  Display results  ####

('Alphas = ', array([  2.01143781e-08,   3.37634087e-10,   1.22603959e-08,
         1.18265155e-10,   3.59732974e-08,   2.10351144e-10,
         6.98866585e-10,   1.26638211e-10]))
('w = ', array([  2.68387380e-04,   7.71822025e-08,   9.82697081e-08,
        -2.21030168e-07,   1.03360261e-07,   1.20294132e-07,
        -4.78593969e-08,   4.98864112e-08,  -1.56416389e-08,
        -3.83184446e-08,  -1.01015505e-07,   9.27079146e-08,
        -1.71764013e-07,   4.69967942e-08,  -2.05841178e-07,
        -2.01301313e-08,   1.20356548e-07,   2.31395762e-07,
         1.26432946e-07,  -1.17474635e-07,  -5.45262617e-09,
        -1.34794104e-08,  -3.61113112e-08,  -2.68412413e-08,
        -6.93643875e-08,   6.11404393e-08,   2.61866582e-08,
         3.13419730e-09,   1.04576625e-08,  -2.11706356e-06]))
('b = ', array([-8.30951492]))

####  Testing Data  ####

#### Accuracy: 93.3333333333
[Finished in 0.3s]  
\end{verbatim}

\begin{itemize}
    \item \textbf{ Got an accuracy of 93.3 \% for dataset 1 }
\end{itemize}
\subsection{Observation}


 \end{document}
\grid
